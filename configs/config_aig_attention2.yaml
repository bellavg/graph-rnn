# Config: Node Attention GRU + standard RNN Edge
data:
  dataset_type: 'aig-directed-multiclass'
  graph_file: 'dataset/final_data.pkl'
  use_bfs: false
  max_graphs: null
  m: 88

model:
  mode: 'directed-multiclass'
  use_lstm: false           # GRU Node
  use_attention: true       # Node Attention
  edge_model: 'rnn'         # Standard RNN Edge

  GraphAttentionRNN:        # Used because use_attention: true, use_lstm: false
    embedding_size: 384
    hidden_size: 768
    num_layers: 4
    edge_feature_len: 3
    attention_heads: 12     # Node attention heads
    attention_dropout: 0.1
    output_size: 256        # MUST match EdgeRNN.hidden_size

  EdgeRNN:                  # Standard EdgeRNN
    embedding_size: 128
    hidden_size: 256        # MUST match GraphAttentionRNN.output_size
    num_layers: 3
    edge_feature_len: 3

train:
  batch_size: 32
  lr: 0.0005                # Lower LR suggested for attention
  steps: 75000              # Total training steps
  print_iter: 100
  checkpoint_iter: 7500
  checkpoint_dir: 'checkpoints/checkpoints_gru_mhsa2' # Specific directory
  log_file: 'train_results/logs_gru_mhsa2'               # Specific directory
  # lr_schedule_* removed; code uses CosineAnnealingLR