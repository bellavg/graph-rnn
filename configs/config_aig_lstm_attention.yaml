# Config: Node Attention LSTM + Edge Attention LSTM
data:
  dataset_type: 'aig-directed-multiclass'
  graph_file: 'dataset/final_data.pkl'
  use_bfs: false
  max_graphs: null
  m: 88

model:
  mode: 'directed-multiclass'
  use_lstm: true          # LSTM Node
  use_attention: true     # Node Attention
  edge_model: 'attention_rnn' # Use Attention Edge Model (maps to EdgeAttentionLSTM)

  GraphAttentionLSTM:     # Used because use_lstm: true, use_attention: true
    embedding_size: 384
    hidden_size: 768
    num_layers: 3
    edge_feature_len: 3
    attention_heads: 8      # Node attention heads
    attention_dropout: 0.1
    output_size: 256        # MUST match EdgeAttentionLSTM.hidden_size

  EdgeAttentionLSTM:      # Used because edge_model: 'attention_rnn' and use_lstm: true
    embedding_size: 128
    hidden_size: 256        # MUST match GraphAttentionLSTM.output_size
    num_layers: 4
    edge_feature_len: 3
    attention_heads: 4      # Edge attention heads
    attention_dropout: 0.1

train:
  batch_size: 32
  lr: 0.0003                # Lower LR suggested for attention
  steps: 80000              # Total training steps
  print_iter: 100
  checkpoint_iter: 5000
  checkpoint_dir: 'checkpoints/checkpoints_lstm_mhsa' # Specific directory
  log_file: 'train_results/logs_lstm_mhsa'               # Specific directory
  # lr_schedule_* removed; code uses CosineAnnealingLR