============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
Using config file: configs/config_aig_rnn.yaml
Loaded config:
data:
  dataset_type: aig-directed-multiclass
  graph_file: dataset/final_data.pkl
  use_bfs: false
  max_graphs: null
model:
  mode: directed-multiclass
  use_lstm: false
  use_attention: false
  edge_model: rnn
  GraphRNN:
    embedding_size: 384
    hidden_size: 768
    num_layers: 4
    edge_feature_len: 3
    output_size: 384
  EdgeRNN:
    embedding_size: 192
    hidden_size: 384
    num_layers: 4
    edge_feature_len: 3
train:
  batch_size: 32
  lr: 0.0005
  steps: 75000
  print_iter: 100
  checkpoint_iter: 7500
  checkpoint_dir: checkpoints/checkpoints_gru_rnn
  log_file: train_results/logs_gru_rnn

Computing maximum node count from dataset...
Maximum node count: 89
Computing maximum level from dataset...
Calculated max level 18 from 37000 valid graphs
Maximum level: 18
Loading AIG graphs from dataset/final_data.pkl...
Limited to 30000 graphs for processing.
Preprocessing graphs...
Graph preprocessing complete. 30000 graphs processed, 0 skipped.
Maximum node count in processed dataset: 89
INFO: Topological Sort mode. Effective input size (m_internal): 88
Calculating node levels...
Maximum node level across dataset: 18
Calculating edge type counts for class weighting...
Calculating weights based on 27000 training graphs...
Total edge slots considered for weights: 98341408
Raw edge counts: {0: 96753923, 1: 959297, 2: 628188}
Calculated edge weights (Inverse Frequency): [0.011724252253770828, 1.1824986934661865, 1.8057770729064941]
Dataset ready: 27000 graphs (training split).
Total available graphs: 30000
Training graphs: 27000 (limited by split ratio)
Testing graphs: 3000
Ordering: Topological Sort
Dataset initialized with 27000 training graphs
Final statistics - Max node count: 89, Max level: 18
INFO: Using Topological Sort mode. Effective input/output size (max_nodes-1): 88
INFO: GraphLevelRNN (No Attention) using level embedding up to level 18
INFO: Using standard GraphLevelRNN for node level.
Selected EdgeLevelRNN model.
Setting up CrossEntropyLoss for 3 edge classes.
Applying edge class weights: [0.011724252253770828, 1.1824986934661865, 1.8057770729064941]
Logger initialized. Logging to train_results/logs_gru_rnn
Logger initialized. Writing to: train_results/logs_gru_rnn
[100/75000] loss=0.8881 lr=5.0E-04 time/iter=0.134s eta=2:47:01
[200/75000] loss=0.8653 lr=5.0E-04 time/iter=0.132s eta=2:44:54
[300/75000] loss=0.8362 lr=5.0E-04 time/iter=0.131s eta=2:43:06
[400/75000] loss=0.8169 lr=5.0E-04 time/iter=0.131s eta=2:42:42
[500/75000] loss=0.8054 lr=5.0E-04 time/iter=0.131s eta=2:42:33
[600/75000] loss=0.7975 lr=5.0E-04 time/iter=0.131s eta=2:42:02
[700/75000] loss=0.7915 lr=5.0E-04 time/iter=0.130s eta=2:41:25
[800/75000] loss=0.7879 lr=5.0E-04 time/iter=0.130s eta=2:40:53
Epoch 1 complete. Average loss: 0.7863
[900/75000] loss=0.7546 lr=5.0E-04 time/iter=0.130s eta=2:40:25
[1000/75000] loss=0.7597 lr=5.0E-04 time/iter=0.130s eta=2:40:07
[1100/75000] loss=0.7594 lr=5.0E-04 time/iter=0.130s eta=2:39:46
[1200/75000] loss=0.7588 lr=5.0E-04 time/iter=0.130s eta=2:39:42
[1300/75000] loss=0.7589 lr=5.0E-04 time/iter=0.130s eta=2:39:31
[1400/75000] loss=0.7580 lr=5.0E-04 time/iter=0.130s eta=2:39:06
[1500/75000] loss=0.7571 lr=5.0E-04 time/iter=0.130s eta=2:38:49
[1600/75000] loss=0.7559 lr=5.0E-04 time/iter=0.130s eta=2:38:33
Epoch 2 complete. Average loss: 0.7555
[1700/75000] loss=0.7469 lr=5.0E-04 time/iter=0.130s eta=2:38:17
[1800/75000] loss=0.7476 lr=5.0E-04 time/iter=0.130s eta=2:38:02
[1900/75000] loss=0.7521 lr=5.0E-04 time/iter=0.130s eta=2:37:47
[2000/75000] loss=0.7522 lr=5.0E-04 time/iter=0.129s eta=2:37:32
[2100/75000] loss=0.7516 lr=5.0E-04 time/iter=0.129s eta=2:37:18
[2200/75000] loss=0.7506 lr=5.0E-04 time/iter=0.129s eta=2:37:03
[2300/75000] loss=0.7500 lr=5.0E-04 time/iter=0.129s eta=2:36:46
[2400/75000] loss=0.7498 lr=5.0E-04 time/iter=0.129s eta=2:36:39
[2500/75000] loss=0.7491 lr=5.0E-04 time/iter=0.129s eta=2:36:25
Epoch 3 complete. Average loss: 0.7489
[2600/75000] loss=0.7432 lr=5.0E-04 time/iter=0.129s eta=2:36:11
[2700/75000] loss=0.7427 lr=5.0E-04 time/iter=0.129s eta=2:35:52
[2800/75000] loss=0.7430 lr=5.0E-04 time/iter=0.129s eta=2:35:43
[2900/75000] loss=0.7421 lr=5.0E-04 time/iter=0.129s eta=2:35:26
[3000/75000] loss=0.7417 lr=5.0E-04 time/iter=0.129s eta=2:35:16
[3100/75000] loss=0.7400 lr=5.0E-04 time/iter=0.129s eta=2:35:04
[3200/75000] loss=0.7348 lr=5.0E-04 time/iter=0.129s eta=2:34:50
[3300/75000] loss=0.7182 lr=5.0E-04 time/iter=0.129s eta=2:34:37
Epoch 4 complete. Average loss: 0.6921
[3400/75000] loss=0.3666 lr=5.0E-04 time/iter=0.129s eta=2:34:24
[3500/75000] loss=0.3333 lr=5.0E-04 time/iter=0.129s eta=2:34:07
[3600/75000] loss=0.3176 lr=5.0E-04 time/iter=0.129s eta=2:33:54
[3700/75000] loss=0.3014 lr=5.0E-04 time/iter=0.129s eta=2:33:41
[3800/75000] loss=0.2957 lr=5.0E-04 time/iter=0.129s eta=2:33:28
[3900/75000] loss=0.2903 lr=5.0E-04 time/iter=0.129s eta=2:33:16
[4000/75000] loss=0.2863 lr=5.0E-04 time/iter=0.129s eta=2:33:04
[4100/75000] loss=0.2824 lr=5.0E-04 time/iter=0.129s eta=2:32:53
[4200/75000] loss=0.2807 lr=5.0E-04 time/iter=0.129s eta=2:32:39
Epoch 5 complete. Average loss: 0.2800
[4300/75000] loss=0.2530 lr=5.0E-04 time/iter=0.129s eta=2:32:26
[4400/75000] loss=0.2553 lr=5.0E-04 time/iter=0.129s eta=2:32:11
[4500/75000] loss=0.2507 lr=5.0E-04 time/iter=0.129s eta=2:31:58
[4600/75000] loss=0.2505 lr=5.0E-04 time/iter=0.129s eta=2:31:45
[4700/75000] loss=0.2471 lr=5.0E-04 time/iter=0.129s eta=2:31:29
[4800/75000] loss=0.2512 lr=4.9E-04 time/iter=0.129s eta=2:31:14
[4900/75000] loss=0.2496 lr=4.9E-04 time/iter=0.129s eta=2:31:02
[5000/75000] loss=0.2489 lr=4.9E-04 time/iter=0.129s eta=2:30:52
Epoch 6 complete. Average loss: 0.2481
[5100/75000] loss=0.2323 lr=4.9E-04 time/iter=0.129s eta=2:30:38
[5200/75000] loss=0.2411 lr=4.9E-04 time/iter=0.129s eta=2:30:26
[5300/75000] loss=0.2402 lr=4.9E-04 time/iter=0.129s eta=2:30:11
[5400/75000] loss=0.2413 lr=4.9E-04 time/iter=0.129s eta=2:29:59
[5500/75000] loss=0.2394 lr=4.9E-04 time/iter=0.129s eta=2:29:46
[5600/75000] loss=0.2387 lr=4.9E-04 time/iter=0.129s eta=2:29:34
[5700/75000] loss=0.2367 lr=4.9E-04 time/iter=0.129s eta=2:29:23
[5800/75000] loss=0.2351 lr=4.9E-04 time/iter=0.129s eta=2:29:10
[5900/75000] loss=0.2350 lr=4.9E-04 time/iter=0.129s eta=2:28:56
Epoch 7 complete. Average loss: 0.2350
[6000/75000] loss=0.2304 lr=4.9E-04 time/iter=0.129s eta=2:28:42
[6100/75000] loss=0.2349 lr=4.9E-04 time/iter=0.129s eta=2:28:28
[6200/75000] loss=0.2316 lr=4.9E-04 time/iter=0.129s eta=2:28:14
[6300/75000] loss=0.2107 lr=4.9E-04 time/iter=0.129s eta=2:28:01
[6400/75000] loss=0.1925 lr=4.9E-04 time/iter=0.129s eta=2:27:49
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 11084001 ON gcn55 CANCELLED AT 2025-04-08T14:25:28 ***
slurmstepd: error: *** STEP 11084001.0 ON gcn55 CANCELLED AT 2025-04-08T14:25:28 ***

JOB STATISTICS
==============
Job ID: 11084001
Cluster: snellius
User/Group: igardner1/igardner1
State: CANCELLED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:17:00
CPU Efficiency: 5.47% of 05:10:48 core-walltime
Job Wall-clock time: 00:17:16
Memory Utilized: 7.52 GB
Memory Efficiency: 6.26% of 120.00 GB (120.00 GB/node)
